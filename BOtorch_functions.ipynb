{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c257aa",
   "metadata": {},
   "source": [
    "# botorch_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b51a54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'fit_gpytorch_torch' from 'botorch.fit' (c:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\botorch\\fit.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKMP_DUPLICATE_LIB_OK\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fit_gpytorch_torch\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SingleTaskGP\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hartmann\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'fit_gpytorch_torch' from 'botorch.fit' (c:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\botorch\\fit.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "from botorch.fit import fit_gpytorch_torch\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.test_functions import Hartmann\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.generation import get_best_candidates, gen_candidates_torch\n",
    "from botorch.optim import gen_batch_initial_conditions\n",
    "from botorch.acquisition import qExpectedImprovement, qUpperConfidenceBound\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, NoReturn, Optional, Tuple, Type, Union\n",
    "from botorch.acquisition.acquisition import AcquisitionFunction\n",
    "from botorch.acquisition.objective import (\n",
    "    IdentityMCObjective,\n",
    "    MCAcquisitionObjective,\n",
    "    PosteriorTransform,\n",
    ")\n",
    "from botorch.models.model import Model\n",
    "from botorch.sampling.samplers import MCSampler, SobolQMCNormalSampler\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "from botorch.utils.transforms import (\n",
    "    concatenate_pending_points,\n",
    "    match_batch_shape,\n",
    "    t_batch_mode_transform,\n",
    ")\n",
    "from botorch.acquisition.monte_carlo import MCAcquisitionFunction\n",
    "from inspect import signature\n",
    "\n",
    "\n",
    "import time\n",
    "from typing import Any, Dict, List, NamedTuple, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from gpytorch.mlls.marginal_log_likelihood import MarginalLogLikelihood\n",
    "from torch import Tensor\n",
    "from torch.optim.adam import Adam\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "def check_convergence(\n",
    "    loss_trajectory: List[float],\n",
    "    param_trajectory: Dict[str, List[Tensor]],\n",
    "    options: Dict[str, Union[float, str]],\n",
    ") -> bool:\n",
    "    r\"\"\"Check convergence of optimization for pytorch optimizers.\n",
    "\n",
    "    Right now this is just a dummy function and only checks for maxiter.\n",
    "\n",
    "    Args:\n",
    "        loss_trajectory: A list containing the loss value at each iteration.\n",
    "        param_trajectory: A dictionary mapping each parameter name to a list of Tensors\n",
    "            where the `i`th Tensor is the parameter value at iteration `i`.\n",
    "        options: dictionary of options. Currently only \"maxiter\" is supported.\n",
    "\n",
    "    Returns:\n",
    "        A boolean indicating whether optimization has converged.\n",
    "    \"\"\"\n",
    "    maxiter: int = options.get(\"maxiter\", 50)\n",
    "    # TODO: Be A LOT smarter about this\n",
    "    # TODO: Make this work in batch mode (see parallel L-BFGS-P)\n",
    "    if len(loss_trajectory) >= maxiter:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def _filter_kwargs(function: Callable, **kwargs: Any) -> Any:\n",
    "    r\"\"\"Filter out kwargs that are not applicable for a given function.\n",
    "    Return a copy of given kwargs dict with only the required kwargs.\"\"\"\n",
    "    return {k: v for k, v in kwargs.items() if k in signature(function).parameters}\n",
    "\n",
    "def _get_extra_mll_args(\n",
    "    mll: MarginalLogLikelihood\n",
    ") -> Union[List[Tensor], List[List[Tensor]]]:\n",
    "    r\"\"\"Obtain extra arguments for MarginalLogLikelihood objects.\n",
    "\n",
    "    Get extra arguments (beyond the model output and training targets) required\n",
    "    for the particular type of MarginalLogLikelihood for a forward pass.\n",
    "\n",
    "    Args:\n",
    "        mll: The MarginalLogLikelihood module.\n",
    "\n",
    "    Returns:\n",
    "        Extra arguments for the MarginalLogLikelihood.\n",
    "    \"\"\"\n",
    "    if isinstance(mll, ExactMarginalLogLikelihood):\n",
    "        return list(mll.model.train_inputs)\n",
    "    elif isinstance(mll, SumMarginalLogLikelihood):\n",
    "        return [list(x) for x in mll.model.train_inputs]\n",
    "    elif isinstance(mll, VariationalELBO):\n",
    "        return []\n",
    "    else:\n",
    "        raise ValueError(\"Do not know how to optimize MLL type.\")\n",
    "\n",
    "\n",
    "\n",
    "ParameterBounds = Dict[str, Tuple[Optional[float], Optional[float]]]\n",
    "\n",
    "\n",
    "class OptimizationIteration(NamedTuple):\n",
    "    itr: int\n",
    "    fun: float\n",
    "    time: float\n",
    "\n",
    "\n",
    "\n",
    "def fit_gpytorch_torch(\n",
    "    mll: MarginalLogLikelihood,\n",
    "    bounds: Optional[ParameterBounds] = None,\n",
    "    optimizer_cls: Optimizer = Adam,\n",
    "    options: Optional[Dict[str, Any]] = None,\n",
    "    track_iterations: bool = True,\n",
    ") -> Tuple[MarginalLogLikelihood, List[OptimizationIteration]]:\n",
    "    r\"\"\"Fit a gpytorch model by maximizing MLL with a torch optimizer.\n",
    "\n",
    "    The model and likelihood in mll must already be in train mode.\n",
    "    Note: this method requires that the model has `train_inputs` and `train_targets`.\n",
    "\n",
    "    Args:\n",
    "        mll: MarginalLogLikelihood to be maximized.\n",
    "        bounds: A ParameterBounds dictionary mapping parameter names to tuples\n",
    "            of lower and upper bounds. Bounds specified here take precedence\n",
    "            over bounds on the same parameters specified in the constraints\n",
    "            registered with the module.\n",
    "        optimizer_cls: Torch optimizer to use. Must not require a closure.\n",
    "        options: options for model fitting. Relevant options will be passed to\n",
    "            the `optimizer_cls`. Additionally, options can include: \"disp\"\n",
    "            to specify whether to display model fitting diagnostics and \"maxiter\"\n",
    "            to specify the maximum number of iterations.\n",
    "        track_iterations: Track the function values and wall time for each\n",
    "            iteration.\n",
    "\n",
    "    Returns:\n",
    "        2-element tuple containing\n",
    "\n",
    "        - mll with parameters optimized in-place.\n",
    "        - List of OptimizationIteration objects with information on each\n",
    "          iteration. If track_iterations is False, this will be an empty list.\n",
    "\n",
    "    Example:\n",
    "        >>> gp = SingleTaskGP(train_X, train_Y)\n",
    "        >>> mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "        >>> mll.train()\n",
    "        >>> fit_gpytorch_torch(mll)\n",
    "        >>> mll.eval()\n",
    "    \"\"\"\n",
    "    optim_options = {\"maxiter\": 100, \"disp\": True, \"lr\": 0.05}\n",
    "    optim_options.update(options or {})\n",
    "    optimizer = optimizer_cls(\n",
    "        params=[{\"params\": mll.parameters()}],\n",
    "        **_filter_kwargs(optimizer_cls, **optim_options),\n",
    "    )\n",
    "\n",
    "    # get bounds specified in model (if any)\n",
    "    bounds_: ParameterBounds = {}\n",
    "    if hasattr(mll, \"named_parameters_and_constraints\"):\n",
    "        for param_name, _, constraint in mll.named_parameters_and_constraints():\n",
    "            if constraint is not None and not constraint.enforced:\n",
    "                bounds_[param_name] = constraint.lower_bound, constraint.upper_bound\n",
    "\n",
    "    # update with user-supplied bounds (overwrites if already exists)\n",
    "    if bounds is not None:\n",
    "        bounds_.update(bounds)\n",
    "\n",
    "    iterations = []\n",
    "    t1 = time.time()\n",
    "\n",
    "    param_trajectory: Dict[str, List[Tensor]] = {\n",
    "        name: [] for name, param in mll.named_parameters()\n",
    "    }\n",
    "    loss_trajectory: List[float] = []\n",
    "    i = 0\n",
    "    converged = False\n",
    "    train_inputs, train_targets = mll.model.train_inputs, mll.model.train_targets\n",
    "    while not converged:\n",
    "        optimizer.zero_grad()\n",
    "        output = mll.model(*train_inputs)\n",
    "\n",
    "        # we sum here to support batch mode\n",
    "        args = [output, train_targets] + _get_extra_mll_args(mll)\n",
    "        loss = -mll(*args).sum()\n",
    "        loss.backward()\n",
    "        loss_trajectory.append(loss.item())\n",
    "        for name, param in mll.named_parameters():\n",
    "            param_trajectory[name].append(param.detach().clone())\n",
    "        if optim_options[\"disp\"] and (\n",
    "            (i + 1) % 10 == 0 or i == (optim_options[\"maxiter\"] - 1)\n",
    "        ):\n",
    "            print(f\"Iter {i + 1}/{optim_options['maxiter']}: {loss.item()}\")\n",
    "        if track_iterations:\n",
    "            iterations.append(OptimizationIteration(i, loss.item(), time.time() - t1))\n",
    "        optimizer.step()\n",
    "        # project onto bounds:\n",
    "        if bounds_:\n",
    "            for pname, param in mll.named_parameters():\n",
    "                if pname in bounds_:\n",
    "                    param.data = param.data.clamp(*bounds_[pname])\n",
    "        i += 1\n",
    "        converged = check_convergence(\n",
    "            loss_trajectory=loss_trajectory,\n",
    "            param_trajectory=param_trajectory,\n",
    "            options={\"maxiter\": optim_options[\"maxiter\"]},\n",
    "        )\n",
    "    return mll, iterations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clamp_tensor(tensor: torch.Tensor, bounds: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Clamps the values of a tensor within given bounds.\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor): The tensor to clamp.\n",
    "        bounds (Tensor): A tensor containing the lower and upper bounds.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The clamped tensor.\n",
    "    \"\"\"\n",
    "    if bounds.dim() == 2 and bounds.size(0) == 2:\n",
    "        # Common bounds for all samples: bounds shape is (2, dim)\n",
    "        lower_bounds, upper_bounds = bounds\n",
    "    elif bounds.dim() == 3 and bounds.size(1) == 2:\n",
    "        # Per-sample bounds: bounds shape is (batch_size, 2, dim)\n",
    "        lower_bounds = bounds[:, 0, :]\n",
    "        upper_bounds = bounds[:, 1, :]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid bounds dimension. Expected bounds of shape (2, dim) or (batch_size, 2, dim).\")\n",
    "    \n",
    "    # Perform clamping\n",
    "    return torch.max(torch.min(tensor, upper_bounds), lower_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09abbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_initial_candidates(bounds: torch.Tensor, num_candidates: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generates initial candidate points uniformly within the given bounds.\n",
    "\n",
    "    Args:\n",
    "        bounds (Tensor): A tensor containing the lower and upper bounds.\n",
    "            - If bounds has shape (2, dim), it represents common bounds for all samples.\n",
    "            - If bounds has shape (batch_size, 2, dim), it represents per-sample bounds.\n",
    "        num_candidates (int): The number of candidates to generate.\n",
    "            - For per-sample bounds, num_candidates must equal batch_size.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A tensor of initial candidate points with shape (num_candidates, dim).\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if bounds.dim() == 2 and bounds.size(0) == 2:\n",
    "        # Common bounds for all samples\n",
    "        dim = bounds.shape[1]\n",
    "        lower_bounds, upper_bounds = bounds  # Shapes: (dim,)\n",
    "        # Generate random points uniformly within bounds\n",
    "        random_numbers = torch.rand(num_candidates, dim)\n",
    "        initial_points = lower_bounds + (upper_bounds - lower_bounds) * random_numbers\n",
    "    elif bounds.dim() == 3 and bounds.size(1) == 2:\n",
    "        # Per-sample bounds\n",
    "        batch_size = bounds.shape[0]\n",
    "        dim = bounds.shape[2]\n",
    "        if num_candidates != batch_size:\n",
    "            raise ValueError(\"For per-sample bounds, num_candidates must equal batch_size.\")\n",
    "        lower_bounds = bounds[:, 0, :]  # Shape: (batch_size, dim)\n",
    "        upper_bounds = bounds[:, 1, :]  # Shape: (batch_size, dim)\n",
    "        # Generate random points uniformly within per-sample bounds\n",
    "        random_numbers = torch.rand(num_candidates, dim)\n",
    "        initial_points = lower_bounds + (upper_bounds - lower_bounds) * random_numbers\n",
    "    else:\n",
    "        raise ValueError(\"Invalid bounds shape. Expected shape (2, dim) or (batch_size, 2, dim).\")\n",
    "    return initial_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee5e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_acquisition_function(\n",
    "    acquisition_function: AcquisitionFunction,\n",
    "    bounds: Tensor,\n",
    "    num_restarts: int,\n",
    "    raw_samples: int,\n",
    "    num_candidates: int,\n",
    "    optimizer_options: Optional[Dict[str, Any]] = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Optimizes the acquisition function to find the next set of candidates.\n",
    "\n",
    "    Args:\n",
    "        acquisition_function (AcquisitionFunction): The acquisition function to optimize.\n",
    "        bounds (Tensor): A tensor containing the lower and upper bounds.\n",
    "        num_restarts (int): Number of restarts for optimization.\n",
    "        raw_samples (int): Number of raw samples to use for initialization.\n",
    "        num_candidates (int): Number of candidates to generate.\n",
    "        optimizer_options (Optional[Dict[str, Any]]): Options for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The optimized candidate points.\n",
    "    \"\"\"\n",
    "    from botorch.optim.optimize import optimize_acqf\n",
    "\n",
    "    # Use BoTorch's built-in optimizer for acquisition functions\n",
    "    # https://botorch.org/api/optim.html\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acquisition_function,\n",
    "        bounds=bounds,\n",
    "        q=num_candidates,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "        options=optimizer_options or {},\n",
    "    )\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "852ca1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_acquisition_function(\n",
    "    acquisition_function: AcquisitionFunction,\n",
    "    bounds: Tensor,\n",
    "    num_restarts: int,\n",
    "    raw_samples: int,\n",
    "    num_candidates: int,\n",
    "    optimizer_options: Optional[Dict[str, Any]] = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Optimizes the acquisition function to find the next set of candidates.\n",
    "\n",
    "    Args:\n",
    "        acquisition_function (AcquisitionFunction): The acquisition function to optimize.\n",
    "        bounds (Tensor): A tensor containing the lower and upper bounds.\n",
    "        num_restarts (int): Number of restarts for optimization.\n",
    "        raw_samples (int): Number of raw samples to use for initialization.\n",
    "        num_candidates (int): Number of candidates to generate.\n",
    "        optimizer_options (Optional[Dict[str, Any]]): Options for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The optimized candidate points.\n",
    "    \"\"\"\n",
    "    from botorch.optim.optimize import optimize_acqf\n",
    "\n",
    "    # Use BoTorch's built-in optimizer for acquisition functions\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acquisition_function,\n",
    "        bounds=bounds,\n",
    "        q=num_candidates,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "        options=optimizer_options or {},\n",
    "    )\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c57a067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_step_bayesian_optimization(\n",
    "    train_x: Tensor,\n",
    "    train_y: Tensor,\n",
    "    bounds: Tensor,\n",
    "    num_candidates: int,\n",
    "    acquisition_function_type: str = 'EI',\n",
    "    acq_function_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    optimizer_options: Optional[Dict[str, Any]] = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Performs one step of Bayesian Optimization to propose new candidate points.\n",
    "\n",
    "    Args:\n",
    "        train_x (Tensor): Training input data.\n",
    "        train_y (Tensor): Training target data.\n",
    "        bounds (Tensor): A tensor containing the lower and upper bounds.\n",
    "        num_candidates (int): Number of candidates to propose.\n",
    "        acquisition_function_type (str): Type of acquisition function ('EI' or 'UCB').\n",
    "        acq_function_kwargs (Optional[Dict[str, Any]]): Additional arguments for the acquisition function.\n",
    "        optimizer_options (Optional[Dict[str, Any]]): Options for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The proposed candidate points.\n",
    "    \"\"\"\n",
    "    # Fit a Gaussian Process model\n",
    "    model = SingleTaskGP(train_x, train_y)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    \n",
    "\n",
    "    \n",
    "    fit_gpytorch_torch(mll)\n",
    "\n",
    "    # Define the acquisition function\n",
    "    if acquisition_function_type == 'EI':\n",
    "        acq_func = qExpectedImprovement(\n",
    "            model=model,\n",
    "            best_f=train_y.max(),\n",
    "            sampler=SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "            **(acq_function_kwargs or {})\n",
    "        )\n",
    "    elif acquisition_function_type == 'UCB':\n",
    "        acq_func = qUpperConfidenceBound(\n",
    "            model=model,\n",
    "            beta=acq_function_kwargs.get('beta', 0.1),\n",
    "            sampler=SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported acquisition function type.\")\n",
    "\n",
    "    # Optimize the acquisition function\n",
    "    candidates = optimize_acquisition_function(\n",
    "        acquisition_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        num_restarts=10,\n",
    "        raw_samples=100,\n",
    "        num_candidates=num_candidates,\n",
    "        optimizer_options=optimizer_options,\n",
    "    )\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "311fa913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomExpectedImprovement(MCAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        best_f: Union[float, Tensor],\n",
    "        xi: float = 0.0,\n",
    "        sampler: Optional[MCSampler] = None,\n",
    "        objective: Optional[MCAcquisitionObjective] = None,\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            sampler=sampler or SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "            objective=objective,\n",
    "            posterior_transform=posterior_transform,\n",
    "        )\n",
    "        self.best_f = best_f\n",
    "        self.xi = xi\n",
    "\n",
    "    @concatenate_pending_points\n",
    "    @t_batch_mode_transform()\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        posterior = self.model.posterior(X)\n",
    "        samples = self.sampler(posterior)\n",
    "        obj = self.objective(samples) if self.objective else samples\n",
    "        improvement = obj - self.best_f - self.xi\n",
    "        return improvement.clamp_min(0).mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be64158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUpperConfidenceBound(MCAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        beta: float = 0.1,\n",
    "        sampler: Optional[MCSampler] = None,\n",
    "        objective: Optional[MCAcquisitionObjective] = None,\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            sampler=sampler or SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "            objective=objective,\n",
    "            posterior_transform=posterior_transform,\n",
    "        )\n",
    "        self.beta = beta\n",
    "\n",
    "    @concatenate_pending_points\n",
    "    @t_batch_mode_transform()\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        posterior = self.model.posterior(X)\n",
    "        samples = self.sampler(posterior)\n",
    "        obj = self.objective(samples) if self.objective else samples\n",
    "        mean = obj.mean(dim=0)\n",
    "        std_dev = obj.std(dim=0)\n",
    "        return mean + self.beta * std_dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c6e5a",
   "metadata": {},
   "source": [
    "# botorch_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312015ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# botorch_fit.py\n",
    "\n",
    "import time\n",
    "from typing import Optional, Dict, Any, List, NamedTuple, Tuple\n",
    "import torch\n",
    "from torch.optim import Optimizer, Adam\n",
    "from gpytorch.mlls.marginal_log_likelihood import MarginalLogLikelihood\n",
    "from gpytorch import settings as gpt_settings\n",
    "\n",
    "class OptimizationIteration(NamedTuple):\n",
    "    iteration: int\n",
    "    loss: float\n",
    "    wall_time: float\n",
    "\n",
    "def fit_gpytorch_torch(\n",
    "    mll: MarginalLogLikelihood,\n",
    "    optimizer_cls: Optimizer = Adam,\n",
    "    options: Optional[Dict[str, Any]] = None,\n",
    "    track_iterations: bool = True,\n",
    "    approx_mll: bool = True,\n",
    ") -> Tuple[MarginalLogLikelihood, List[OptimizationIteration]]:\n",
    "    \"\"\"\n",
    "    Fits a GPyTorch model by maximizing the Marginal Log Likelihood (MLL) using a PyTorch optimizer.\n",
    "\n",
    "    Args:\n",
    "        mll (MarginalLogLikelihood): The MLL to be maximized.\n",
    "        optimizer_cls (Optimizer): The PyTorch optimizer class to use (default is Adam).\n",
    "        options (Optional[Dict[str, Any]]): Options for model fitting and the optimizer.\n",
    "            Relevant options include:\n",
    "                - \"lr\": Learning rate for the optimizer.\n",
    "                - \"maxiter\": Maximum number of iterations.\n",
    "                - \"disp\": If True, displays optimization progress.\n",
    "        track_iterations (bool): If True, tracks the function values and wall time for each iteration.\n",
    "        approx_mll (bool): If True, uses approximate MLL computation for efficiency.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - mll: The MLL with optimized parameters.\n",
    "            - iterations: A list of OptimizationIteration objects with information on each iteration.\n",
    "    \"\"\"\n",
    "    options = options or {}\n",
    "    lr = options.get(\"lr\", 0.1)\n",
    "    maxiter = options.get(\"maxiter\", 50)\n",
    "    disp = options.get(\"disp\", False)\n",
    "    exclude = options.get(\"exclude\", None)\n",
    "\n",
    "    # Prepare parameters for optimization\n",
    "    if exclude is not None:\n",
    "        mll_params = [\n",
    "            t for p_name, t in mll.named_parameters() if p_name not in exclude\n",
    "        ]\n",
    "    else:\n",
    "        mll_params = list(mll.parameters())\n",
    "\n",
    "    optimizer = optimizer_cls(\n",
    "        params=[{\"params\": mll_params}],\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    iterations = []\n",
    "    t_start = time.time()\n",
    "    param_trajectory: Dict[str, List[torch.Tensor]] = {\n",
    "        name: [] for name, param in mll.named_parameters()\n",
    "    }\n",
    "    loss_trajectory: List[float] = []\n",
    "\n",
    "    mll.train()\n",
    "    train_inputs = mll.model.train_inputs\n",
    "    train_targets = mll.model.train_targets\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        optimizer.zero_grad()\n",
    "        with gpt_settings.fast_computations(log_prob=approx_mll):\n",
    "            output = mll.model(*train_inputs)\n",
    "            # Sum over batch dimensions for compatibility\n",
    "            loss = -mll(output, train_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        loss_trajectory.append(loss_value)\n",
    "        for name, param in mll.named_parameters():\n",
    "            param_trajectory[name].append(param.detach().clone())\n",
    "\n",
    "        if disp and ((i + 1) % 10 == 0 or i == maxiter - 1):\n",
    "            print(f\"Iter {i + 1}/{maxiter}: Loss = {loss_value:.4f}\")\n",
    "\n",
    "        if track_iterations:\n",
    "            iterations.append(\n",
    "                OptimizationIteration(\n",
    "                    iteration=i,\n",
    "                    loss=loss_value,\n",
    "                    wall_time=time.time() - t_start,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    mll.eval()\n",
    "    return mll, iterations, param_trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbaca8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\utils\\cholesky.py:38: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "c:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\utils\\cholesky.py:38: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "c:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\utils\\cholesky.py:38: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotPSDError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m candidates\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Perform a single Bayesian Optimization step\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_bayesian_optimization_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m candidates\n",
      "Cell \u001b[1;32mIn[42], line 52\u001b[0m, in \u001b[0;36msimulate_bayesian_optimization_step\u001b[1;34m(train_x, train_y, bounds, num_candidates)\u001b[0m\n\u001b[0;32m     50\u001b[0m model \u001b[38;5;241m=\u001b[39m SingleTaskGP(train_x, train_y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     51\u001b[0m mll \u001b[38;5;241m=\u001b[39m ExactMarginalLogLikelihood(model\u001b[38;5;241m.\u001b[39mlikelihood, model)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mfit_gpytorch_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmll\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fit GP model to data\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Define Expected Improvement acquisition function\u001b[39;00m\n\u001b[0;32m     55\u001b[0m acq_func \u001b[38;5;241m=\u001b[39m qExpectedImprovement(\n\u001b[0;32m     56\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     57\u001b[0m     best_f\u001b[38;5;241m=\u001b[39mtrain_y\u001b[38;5;241m.\u001b[39mmax(),\n\u001b[0;32m     58\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mSobolQMCNormalSampler(sample_shape\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mSize([\u001b[38;5;241m500\u001b[39m])),\n\u001b[0;32m     59\u001b[0m )\n",
      "Cell \u001b[1;32mIn[8], line 76\u001b[0m, in \u001b[0;36mfit_gpytorch_torch\u001b[1;34m(mll, optimizer_cls, options, track_iterations, approx_mll)\u001b[0m\n\u001b[0;32m     74\u001b[0m     output \u001b[38;5;241m=\u001b[39m mll\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39mtrain_inputs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# Sum over batch dimensions for compatibility\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     77\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     78\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py:62\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[1;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[0;32m     61\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 62\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:169\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[0;32m    168\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[1;32m--> 169\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\lazy\\lazy_tensor.py:1291\u001b[0m, in \u001b[0;36mLazyTensor.inv_quad_logdet\u001b[1;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[0;32m   1289\u001b[0m             will_need_cholesky \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m will_need_cholesky:\n\u001b[1;32m-> 1291\u001b[0m         cholesky \u001b[38;5;241m=\u001b[39m CholLazyTensor(TriangularLazyTensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cholesky\u001b[38;5;241m.\u001b[39minv_quad_logdet(\n\u001b[0;32m   1293\u001b[0m         inv_quad_rhs\u001b[38;5;241m=\u001b[39minv_quad_rhs,\n\u001b[0;32m   1294\u001b[0m         logdet\u001b[38;5;241m=\u001b[39mlogdet,\n\u001b[0;32m   1295\u001b[0m         reduce_inv_quad\u001b[38;5;241m=\u001b[39mreduce_inv_quad,\n\u001b[0;32m   1296\u001b[0m     )\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;66;03m# Default: use modified batch conjugate gradients to compute these terms\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;66;03m# See NeurIPS 2018 paper: https://arxiv.org/abs/1809.11165\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\lazy\\lazy_tensor.py:1004\u001b[0m, in \u001b[0;36mLazyTensor.cholesky\u001b[1;34m(self, upper)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(\u001b[38;5;28mself\u001b[39m, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    995\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LazyTensor\u001b[39;00m\n\u001b[0;32m    997\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;124;03m        (LazyTensor) Cholesky factor (triangular, upper/lower depending on \"upper\" arg)\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1004\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[0;32m   1006\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[1;32mc:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\lazy\\lazy_tensor.py:435\u001b[0m, in \u001b[0;36mLazyTensor._cholesky\u001b[1;34m(self, upper)\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLazyTensor(evaluated_mat\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt())\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# contiguous call is necessary here\u001b[39;00m\n\u001b[1;32m--> 435\u001b[0m cholesky \u001b[38;5;241m=\u001b[39m \u001b[43mpsd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluated_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLazyTensor(cholesky, upper\u001b[38;5;241m=\u001b[39mupper)\n",
      "File \u001b[1;32mc:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\utils\\cholesky.py:63\u001b[0m, in \u001b[0;36mpsd_safe_cholesky\u001b[1;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpsd_safe_cholesky\u001b[39m(A, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m        :attr:`A` (Tensor):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m            Number of attempts (with successively increasing jitter) to make before raising an error.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[43m_psd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\utils\\cholesky.py:45\u001b[0m, in \u001b[0;36m_psd_safe_cholesky\u001b[1;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(info):\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m L\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NotPSDError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatrix not positive definite after repeatedly adding jitter up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjitter_new\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotPSDError\u001b[0m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define simulation parameters\n",
    "num_frames = 100  # Number of frames in the video sequence\n",
    "num_pedestrians = 5  # Number of unique pedestrians\n",
    "\n",
    "# Generate mock data\n",
    "frame_numbers = np.repeat(np.arange(1, num_frames + 1), num_pedestrians)\n",
    "pedestrian_ids = np.tile(np.arange(1, num_pedestrians + 1), num_frames)\n",
    "x_coordinates = np.random.uniform(0, 10, size=num_frames * num_pedestrians)  # Random X coordinates\n",
    "y_coordinates = np.random.uniform(0, 10, size=num_frames * num_pedestrians)  # Random Y coordinates\n",
    "\n",
    "# Create DataFrame\n",
    "pedestrian_data = pd.DataFrame({\n",
    "    \"Frame Number\": frame_numbers,\n",
    "    \"Pedestrian ID\": pedestrian_ids,\n",
    "    \"X-Coordinate\": x_coordinates,\n",
    "    \"Y-Coordinate\": y_coordinates\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "pedestrian_data.head()\n",
    "\n",
    "# Now set up for testing Bayesian optimization on this data\n",
    "# Convert X and Y coordinates as sample input data for optimization function\n",
    "\n",
    "# Convert data to PyTorch tensors for testing\n",
    "train_x = torch.tensor(np.column_stack((x_coordinates, y_coordinates)), dtype=torch.float32)  # Mock input data\n",
    "train_y = torch.sin(train_x[:, 0]) + torch.cos(train_x[:, 1])\n",
    " # Mock target values based on some function of X, Y\n",
    "\n",
    "# Define bounds for optimization - based on the X and Y ranges in the dataset\n",
    "bounds = torch.tensor([\n",
    "    [train_x[:, 0].min(), train_x[:, 1].min()],\n",
    "    [train_x[:, 0].max(), train_x[:, 1].max()]\n",
    "])\n",
    "\n",
    "\n",
    "# Function to simulate Bayesian Optimization step using provided code\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import qExpectedImprovement\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "\n",
    "def simulate_bayesian_optimization_step(train_x, train_y, bounds, num_candidates=2):\n",
    "    # Fit a Gaussian Process model\n",
    "    model = SingleTaskGP(train_x, train_y.unsqueeze(-1))\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_torch(mll)  # Fit GP model to data\n",
    "\n",
    "    # Define Expected Improvement acquisition function\n",
    "    acq_func = qExpectedImprovement(\n",
    "        model=model,\n",
    "        best_f=train_y.max(),\n",
    "        sampler=SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "    )\n",
    "\n",
    "    # Optimize acquisition function to get candidate points\n",
    "    from botorch.optim.optimize import optimize_acqf\n",
    "\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=num_candidates,\n",
    "        num_restarts=5,\n",
    "        raw_samples=20,\n",
    "    )\n",
    "    return candidates\n",
    "\n",
    "# Perform a single Bayesian Optimization step\n",
    "candidates = simulate_bayesian_optimization_step(train_x, train_y, bounds)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "835c874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0037e+00, 3.9438e+00],\n",
      "        [7.5992e-02, 7.6637e+00],\n",
      "        [2.0571e+00, 4.9820e+00],\n",
      "        [8.8302e+00, 6.6378e+00],\n",
      "        [7.8981e+00, 5.1877e+00],\n",
      "        [4.2092e+00, 2.7032e+00],\n",
      "        [8.7940e+00, 5.9457e+00],\n",
      "        [7.6066e+00, 3.5293e+00],\n",
      "        [1.4100e+00, 1.2340e+00],\n",
      "        [7.2056e+00, 1.9346e-02],\n",
      "        [4.4567e+00, 2.7268e+00],\n",
      "        [2.3158e+00, 6.0088e+00],\n",
      "        [2.9476e+00, 9.0799e+00],\n",
      "        [2.9796e+00, 6.9664e+00],\n",
      "        [6.0779e+00, 5.4799e+00],\n",
      "        [2.3583e+00, 7.4453e+00],\n",
      "        [7.6260e-01, 4.6160e+00],\n",
      "        [3.2083e+00, 2.5437e+00],\n",
      "        [9.2057e+00, 7.7529e-01],\n",
      "        [9.5563e+00, 3.9906e+00],\n",
      "        [3.7817e+00, 9.6007e+00],\n",
      "        [9.3980e-01, 6.8965e+00],\n",
      "        [7.1114e-01, 7.1634e+00],\n",
      "        [1.3204e+00, 4.8225e+00],\n",
      "        [5.8688e-01, 8.1805e+00],\n",
      "        [6.9636e+00, 4.1035e+00],\n",
      "        [6.9471e+00, 9.6730e+00],\n",
      "        [1.8348e+00, 4.5265e+00],\n",
      "        [5.7352e-01, 8.5774e+00],\n",
      "        [9.4749e+00, 5.9938e-01],\n",
      "        [2.5301e+00, 7.1635e+00],\n",
      "        [1.7996e-01, 9.5298e+00],\n",
      "        [5.4576e+00, 8.7190e+00],\n",
      "        [5.0947e+00, 8.0783e+00],\n",
      "        [6.0149e+00, 7.9353e+00],\n",
      "        [2.4132e+00, 3.5046e+00],\n",
      "        [2.3333e+00, 4.5231e+00],\n",
      "        [4.8612e+00, 4.7619e+00],\n",
      "        [9.4896e+00, 7.9840e+00],\n",
      "        [5.7718e+00, 4.9310e+00],\n",
      "        [3.8133e+00, 5.5017e+00],\n",
      "        [2.3903e+00, 3.5503e+00],\n",
      "        [2.5783e-01, 7.4697e+00],\n",
      "        [8.4227e+00, 1.5592e+00],\n",
      "        [3.0192e+00, 4.1059e+00],\n",
      "        [9.7000e+00, 5.0076e+00],\n",
      "        [9.8815e+00, 9.1027e+00],\n",
      "        [8.3556e+00, 7.7903e-01],\n",
      "        [4.5251e+00, 8.6873e+00],\n",
      "        [7.6832e-01, 5.1799e+00],\n",
      "        [9.0648e+00, 8.1870e-01],\n",
      "        [1.7010e+00, 8.4195e+00],\n",
      "        [2.8435e+00, 3.5407e+00],\n",
      "        [7.4902e+00, 2.9047e+00],\n",
      "        [2.3336e+00, 9.1729e-01],\n",
      "        [1.2876e+00, 8.2109e-01],\n",
      "        [2.7217e-01, 9.4737e+00],\n",
      "        [2.0535e+00, 4.6368e+00],\n",
      "        [4.2204e+00, 5.5677e+00],\n",
      "        [9.5371e-01, 3.1134e+00],\n",
      "        [5.9386e+00, 3.3989e+00],\n",
      "        [5.2539e+00, 3.0691e+00],\n",
      "        [7.1438e+00, 3.4298e+00],\n",
      "        [6.9687e+00, 7.8557e+00],\n",
      "        [9.8238e+00, 5.1082e+00],\n",
      "        [2.5369e+00, 7.5931e+00],\n",
      "        [2.2275e+00, 1.1779e+00],\n",
      "        [6.7757e+00, 2.9464e+00],\n",
      "        [5.3553e+00, 6.5851e+00],\n",
      "        [5.1063e+00, 5.5962e+00],\n",
      "        [4.1345e+00, 8.3200e+00],\n",
      "        [1.1214e+00, 9.5872e-01],\n",
      "        [1.7631e+00, 8.4013e+00],\n",
      "        [7.7208e+00, 6.6549e+00],\n",
      "        [5.5582e+00, 1.8297e+00],\n",
      "        [3.5070e+00, 1.4050e+00],\n",
      "        [4.4814e+00, 7.6730e+00],\n",
      "        [5.5188e+00, 9.4490e+00],\n",
      "        [5.8117e+00, 6.1723e-01],\n",
      "        [4.0578e-01, 6.1352e+00],\n",
      "        [1.3566e+00, 6.4199e+00],\n",
      "        [9.2023e+00, 8.3751e-01],\n",
      "        [6.6045e+00, 2.2148e+00],\n",
      "        [2.1167e+00, 7.1863e+00],\n",
      "        [9.6003e+00, 8.2217e+00],\n",
      "        [5.1812e+00, 3.5056e+00],\n",
      "        [3.8277e+00, 8.9536e+00],\n",
      "        [6.5993e+00, 6.3576e+00],\n",
      "        [1.6223e+00, 5.6916e+00],\n",
      "        [8.1428e+00, 2.6902e+00],\n",
      "        [2.8138e+00, 9.5578e+00],\n",
      "        [6.8010e+00, 1.1328e+00],\n",
      "        [6.6460e+00, 8.5534e+00],\n",
      "        [2.3109e+00, 2.9491e+00],\n",
      "        [2.4339e-01, 6.7090e+00],\n",
      "        [1.8319e+00, 7.9282e-01],\n",
      "        [6.9745e+00, 2.6511e+00],\n",
      "        [4.4128e+00, 4.3489e-01],\n",
      "        [2.1354e+00, 4.7173e+00],\n",
      "        [9.4934e+00, 3.8260e+00],\n",
      "        [8.2147e+00, 7.7280e+00],\n",
      "        [9.6327e+00, 7.8055e+00],\n",
      "        [9.4061e+00, 5.7259e+00],\n",
      "        [3.0601e+00, 1.2916e+00],\n",
      "        [2.1921e+00, 4.3801e+00],\n",
      "        [5.8942e+00, 6.0091e+00],\n",
      "        [1.3676e-01, 2.4533e+00],\n",
      "        [4.8129e+00, 5.6185e+00],\n",
      "        [8.4240e+00, 4.3225e-01],\n",
      "        [6.2599e+00, 7.6768e+00],\n",
      "        [1.8321e+00, 9.6571e+00],\n",
      "        [4.6495e+00, 8.2321e+00],\n",
      "        [1.3862e+00, 7.7358e-01],\n",
      "        [2.4486e+00, 1.9406e+00],\n",
      "        [4.5448e+00, 8.2007e+00],\n",
      "        [9.9945e+00, 1.4213e+00],\n",
      "        [3.2331e-01, 5.9642e+00],\n",
      "        [2.8047e-01, 5.0598e+00],\n",
      "        [2.4388e+00, 7.6048e-01],\n",
      "        [1.7765e+00, 1.0967e+00],\n",
      "        [1.3303e+00, 6.5547e+00],\n",
      "        [4.2740e+00, 2.5885e-01],\n",
      "        [3.6489e+00, 5.1797e+00],\n",
      "        [1.6744e+00, 6.7198e+00],\n",
      "        [6.2030e+00, 9.1610e-01],\n",
      "        [7.7856e+00, 8.3486e+00],\n",
      "        [4.0812e+00, 6.2698e+00],\n",
      "        [3.0527e+00, 6.4911e-01],\n",
      "        [8.1240e+00, 8.9286e+00],\n",
      "        [9.2218e+00, 8.7780e+00],\n",
      "        [8.9920e+00, 6.8041e+00],\n",
      "        [6.8145e+00, 4.4916e+00],\n",
      "        [8.7472e+00, 4.6162e+00],\n",
      "        [2.8128e+00, 2.2264e+00],\n",
      "        [8.8663e+00, 6.0148e+00],\n",
      "        [3.9377e+00, 7.3214e+00],\n",
      "        [4.8022e+00, 1.5856e+00],\n",
      "        [6.0780e+00, 7.3787e+00],\n",
      "        [2.4649e+00, 8.8030e-01],\n",
      "        [7.0899e+00, 6.1408e+00],\n",
      "        [4.8037e+00, 5.6694e+00],\n",
      "        [2.2302e+00, 4.7223e-01],\n",
      "        [8.3711e+00, 8.4379e+00],\n",
      "        [7.7813e+00, 3.6862e+00],\n",
      "        [8.2588e-01, 1.6971e+00],\n",
      "        [6.3574e+00, 2.8685e+00],\n",
      "        [3.8920e+00, 8.0698e+00],\n",
      "        [9.4916e+00, 5.0264e+00],\n",
      "        [3.2477e+00, 7.4817e+00],\n",
      "        [3.8410e+00, 8.8281e+00],\n",
      "        [5.5837e+00, 3.7844e-01],\n",
      "        [8.8523e+00, 7.2336e+00],\n",
      "        [4.9907e+00, 1.5160e+00],\n",
      "        [1.1356e+00, 7.1670e+00],\n",
      "        [6.8218e+00, 3.8560e+00],\n",
      "        [9.9079e+00, 6.1349e+00],\n",
      "        [4.5528e+00, 7.4706e+00],\n",
      "        [9.3762e+00, 4.6599e+00],\n",
      "        [3.4482e+00, 9.5622e+00],\n",
      "        [7.4856e-01, 7.3421e+00],\n",
      "        [7.2157e+00, 6.2703e+00],\n",
      "        [1.5001e+00, 3.3092e+00],\n",
      "        [4.1265e+00, 3.3752e+00],\n",
      "        [9.3171e+00, 3.6009e+00],\n",
      "        [5.0674e+00, 2.5289e+00],\n",
      "        [5.1029e+00, 9.8635e+00],\n",
      "        [5.4662e+00, 7.8449e+00],\n",
      "        [4.2848e+00, 4.2029e+00],\n",
      "        [7.2904e+00, 6.9238e+00],\n",
      "        [1.8623e+00, 5.0453e+00],\n",
      "        [2.3662e+00, 8.3462e+00],\n",
      "        [3.7311e+00, 9.0554e+00],\n",
      "        [2.9990e+00, 9.2308e+00],\n",
      "        [4.4038e+00, 1.1552e-01],\n",
      "        [9.5026e+00, 4.1904e+00],\n",
      "        [7.1204e+00, 1.8159e+00],\n",
      "        [6.0012e+00, 9.7949e+00],\n",
      "        [9.8234e+00, 2.8973e+00],\n",
      "        [1.9371e+00, 6.7691e+00],\n",
      "        [8.2387e+00, 2.9460e+00],\n",
      "        [5.6574e+00, 1.4782e+00],\n",
      "        [2.8637e+00, 1.8802e+00],\n",
      "        [1.7467e+00, 5.9956e+00],\n",
      "        [8.5942e+00, 5.4664e+00],\n",
      "        [7.3445e+00, 9.4062e+00],\n",
      "        [7.4423e+00, 4.8074e+00],\n",
      "        [3.3139e+00, 3.0685e+00],\n",
      "        [3.5696e+00, 7.6256e+00],\n",
      "        [8.9634e+00, 8.4416e-01],\n",
      "        [8.4037e+00, 5.3653e+00],\n",
      "        [6.3358e+00, 7.1605e+00],\n",
      "        [5.6251e+00, 7.8932e+00],\n",
      "        [1.4794e+00, 4.3167e+00],\n",
      "        [5.0611e+00, 7.0732e+00],\n",
      "        [8.9087e+00, 8.2396e+00],\n",
      "        [3.9327e+00, 7.2151e+00],\n",
      "        [1.1472e+00, 8.6713e+00],\n",
      "        [5.5523e+00, 9.7381e+00],\n",
      "        [4.5886e+00, 5.2658e+00],\n",
      "        [1.0511e-02, 5.9176e+00],\n",
      "        [7.1702e-01, 9.7550e+00],\n",
      "        [3.3505e+00, 2.8278e+00],\n",
      "        [5.4247e+00, 8.0033e-01],\n",
      "        [8.3761e+00, 6.6344e+00],\n",
      "        [9.4589e+00, 6.0726e+00],\n",
      "        [5.2788e+00, 9.6418e+00],\n",
      "        [6.8623e+00, 6.1737e-01],\n",
      "        [3.4183e+00, 2.8488e+00],\n",
      "        [6.6359e+00, 5.3758e-01],\n",
      "        [1.1477e+00, 3.0676e+00],\n",
      "        [6.7613e-01, 4.0631e+00],\n",
      "        [5.5999e+00, 3.7279e-01],\n",
      "        [9.4919e+00, 4.1457e+00],\n",
      "        [1.1751e+00, 3.5546e+00],\n",
      "        [9.3892e+00, 1.7634e+00],\n",
      "        [5.4078e+00, 8.0404e+00],\n",
      "        [2.9283e+00, 2.4935e+00],\n",
      "        [1.5152e+00, 1.8103e+00],\n",
      "        [4.8520e+00, 8.9680e+00],\n",
      "        [4.4867e+00, 2.0389e+00],\n",
      "        [4.3089e+00, 2.9647e+00],\n",
      "        [4.6702e+00, 4.5319e+00],\n",
      "        [6.3997e+00, 5.1726e+00],\n",
      "        [7.3858e+00, 3.6493e+00],\n",
      "        [6.3735e+00, 8.9080e+00],\n",
      "        [1.3141e+00, 4.6248e+00],\n",
      "        [9.1385e+00, 5.2206e+00],\n",
      "        [2.2174e+00, 9.4288e+00],\n",
      "        [9.7286e+00, 2.0342e+00],\n",
      "        [1.2748e+00, 4.8911e-01],\n",
      "        [1.3012e-01, 7.5352e+00],\n",
      "        [8.5511e+00, 1.9798e+00],\n",
      "        [9.0868e+00, 3.3660e+00],\n",
      "        [4.8313e+00, 6.1734e+00],\n",
      "        [1.0412e+00, 6.7162e+00],\n",
      "        [1.6776e+00, 2.0097e+00],\n",
      "        [5.4162e+00, 9.7681e+00],\n",
      "        [1.7711e+00, 8.6340e-01],\n",
      "        [5.7474e+00, 8.9713e-01],\n",
      "        [4.2709e+00, 9.6508e+00],\n",
      "        [1.8582e+00, 3.1427e+00],\n",
      "        [1.7292e+00, 1.8292e+00],\n",
      "        [5.3913e+00, 3.0426e+00],\n",
      "        [8.5864e-01, 8.9535e+00],\n",
      "        [5.3136e+00, 4.7102e+00],\n",
      "        [2.2754e+00, 8.5316e+00],\n",
      "        [3.2015e+00, 2.9640e+00],\n",
      "        [1.5368e-01, 4.0391e+00],\n",
      "        [4.1891e+00, 4.0994e+00],\n",
      "        [4.1144e+00, 7.3059e+00],\n",
      "        [8.7295e+00, 8.2576e+00],\n",
      "        [5.6846e+00, 3.6600e+00],\n",
      "        [7.5927e+00, 4.0309e-01],\n",
      "        [4.8568e+00, 9.8015e+00],\n",
      "        [8.6183e+00, 3.7426e+00],\n",
      "        [4.1581e+00, 6.7317e+00],\n",
      "        [4.9486e+00, 4.7394e+00],\n",
      "        [6.1876e+00, 6.0648e+00],\n",
      "        [6.7183e-01, 4.6548e+00],\n",
      "        [6.8947e+00, 4.3331e+00],\n",
      "        [1.5115e+00, 8.1472e+00],\n",
      "        [8.5142e+00, 2.4689e+00],\n",
      "        [5.2941e+00, 1.3608e+00],\n",
      "        [2.7522e+00, 2.4966e+00],\n",
      "        [1.8127e+00, 9.8624e+00],\n",
      "        [1.3348e+00, 4.5799e+00],\n",
      "        [4.3748e-01, 6.3249e+00],\n",
      "        [7.3177e+00, 9.3733e+00],\n",
      "        [4.3408e+00, 3.5837e+00],\n",
      "        [7.6831e+00, 1.4892e+00],\n",
      "        [1.6927e+00, 7.9310e+00],\n",
      "        [7.3922e+00, 9.8420e-01],\n",
      "        [5.2216e+00, 5.7671e+00],\n",
      "        [1.4823e+00, 8.1297e+00],\n",
      "        [4.4327e+00, 2.0726e+00],\n",
      "        [7.5447e+00, 4.6438e+00],\n",
      "        [7.1572e+00, 2.2521e+00],\n",
      "        [7.3914e+00, 1.6189e+00],\n",
      "        [4.8056e+00, 3.1622e+00],\n",
      "        [3.9630e+00, 3.4423e+00],\n",
      "        [2.3257e+00, 8.5398e-01],\n",
      "        [4.8325e+00, 4.4384e+00],\n",
      "        [2.2601e+00, 1.1407e+00],\n",
      "        [4.0248e+00, 9.4216e+00],\n",
      "        [2.9048e+00, 9.3202e+00],\n",
      "        [9.5338e+00, 8.9997e+00],\n",
      "        [5.1737e+00, 5.0238e+00],\n",
      "        [5.7445e+00, 8.0264e+00],\n",
      "        [7.8193e+00, 1.0058e+00],\n",
      "        [1.1664e+00, 2.5462e-01],\n",
      "        [3.6883e+00, 4.3877e+00],\n",
      "        [4.0018e+00, 5.7208e+00],\n",
      "        [4.1051e+00, 1.4642e+00],\n",
      "        [8.7239e+00, 9.6247e+00],\n",
      "        [4.3101e+00, 9.5437e+00],\n",
      "        [6.9958e-01, 8.1837e+00],\n",
      "        [4.6779e+00, 3.4892e+00],\n",
      "        [2.1471e+00, 9.1187e+00],\n",
      "        [9.3855e+00, 8.2134e+00],\n",
      "        [7.5560e+00, 7.7863e+00],\n",
      "        [2.6391e+00, 8.6295e+00],\n",
      "        [2.2161e+00, 5.8826e-01],\n",
      "        [1.0244e+00, 5.8974e+00],\n",
      "        [1.9013e+00, 8.6561e+00],\n",
      "        [2.7726e+00, 4.6348e+00],\n",
      "        [1.4756e+00, 6.2284e+00],\n",
      "        [1.4032e+00, 8.1292e+00],\n",
      "        [8.9162e+00, 9.6590e+00],\n",
      "        [8.9870e+00, 3.9022e-01],\n",
      "        [4.7923e+00, 6.2905e+00],\n",
      "        [2.7145e+00, 7.2315e+00],\n",
      "        [2.1476e+00, 7.6471e+00],\n",
      "        [8.4092e-01, 8.0496e+00],\n",
      "        [1.5664e+00, 5.1976e+00],\n",
      "        [5.0298e+00, 8.1348e+00],\n",
      "        [7.1530e+00, 6.3686e+00],\n",
      "        [4.4459e+00, 3.0568e+00],\n",
      "        [2.8540e-03, 3.0718e+00],\n",
      "        [8.3119e+00, 9.7469e+00],\n",
      "        [2.8152e-01, 5.5516e+00],\n",
      "        [7.8111e+00, 1.3227e-01],\n",
      "        [5.4830e+00, 2.7867e+00],\n",
      "        [9.8850e+00, 5.9194e+00],\n",
      "        [7.9070e+00, 8.5367e+00],\n",
      "        [3.4295e+00, 3.8419e+00],\n",
      "        [4.1583e+00, 6.5827e-01],\n",
      "        [6.7488e+00, 3.4686e+00],\n",
      "        [8.6161e-01, 3.4140e+00],\n",
      "        [8.6108e+00, 3.9832e+00],\n",
      "        [3.1428e+00, 1.0917e+00],\n",
      "        [5.4062e+00, 6.7592e+00],\n",
      "        [4.9401e+00, 8.3444e+00],\n",
      "        [8.9683e+00, 8.1400e+00],\n",
      "        [6.3614e+00, 1.4208e+00],\n",
      "        [1.0132e+00, 5.0949e+00],\n",
      "        [3.5446e+00, 5.1446e+00],\n",
      "        [7.8049e+00, 9.8661e+00],\n",
      "        [2.0193e-01, 8.6802e+00],\n",
      "        [3.0864e+00, 6.1299e+00],\n",
      "        [2.9766e+00, 7.8213e+00],\n",
      "        [2.6146e+00, 8.7408e+00],\n",
      "        [2.5271e+00, 3.0725e+00],\n",
      "        [9.0020e+00, 9.3988e+00],\n",
      "        [7.0084e+00, 9.3308e+00],\n",
      "        [3.5920e+00, 4.3459e+00],\n",
      "        [3.0144e+00, 1.8203e+00],\n",
      "        [4.8302e+00, 2.2225e+00],\n",
      "        [6.6767e+00, 4.1133e+00],\n",
      "        [2.5660e+00, 4.7950e+00],\n",
      "        [8.1381e+00, 8.5146e+00],\n",
      "        [5.2849e+00, 7.7074e+00],\n",
      "        [5.8795e+00, 4.6917e+00],\n",
      "        [4.7786e+00, 4.1853e-01],\n",
      "        [9.4394e+00, 7.9978e+00],\n",
      "        [9.7396e+00, 6.4826e+00],\n",
      "        [9.3830e+00, 6.7370e+00],\n",
      "        [9.4140e+00, 4.3585e+00],\n",
      "        [2.9044e+00, 3.6778e+00],\n",
      "        [4.2798e+00, 6.2252e+00],\n",
      "        [7.9743e+00, 5.3843e+00],\n",
      "        [2.1087e+00, 2.1125e+00],\n",
      "        [7.5931e+00, 3.6909e+00],\n",
      "        [6.3844e+00, 1.4263e+00],\n",
      "        [5.9062e+00, 4.8390e+00],\n",
      "        [6.6789e+00, 8.9975e-01],\n",
      "        [3.3252e+00, 2.8320e+00],\n",
      "        [7.2134e+00, 5.2062e+00],\n",
      "        [4.0143e+00, 3.7096e+00],\n",
      "        [6.0879e+00, 7.3097e+00],\n",
      "        [5.0264e+00, 6.3225e+00],\n",
      "        [6.3220e+00, 8.0404e+00],\n",
      "        [5.0957e+00, 6.9770e+00],\n",
      "        [9.2447e-01, 8.3305e+00],\n",
      "        [7.1633e+00, 2.0432e+00],\n",
      "        [6.7152e+00, 1.5109e+00],\n",
      "        [2.4487e-01, 6.0219e+00],\n",
      "        [6.0677e+00, 1.2709e+00],\n",
      "        [9.4142e+00, 4.7799e+00],\n",
      "        [2.4803e-01, 5.2266e+00],\n",
      "        [1.6050e+00, 4.3832e+00],\n",
      "        [5.8516e-01, 5.2813e+00],\n",
      "        [9.0555e+00, 1.1093e+00],\n",
      "        [8.7401e+00, 7.9236e+00],\n",
      "        [8.4909e+00, 4.5436e+00],\n",
      "        [9.5193e+00, 9.1990e+00],\n",
      "        [9.1209e+00, 7.3141e+00],\n",
      "        [9.8768e+00, 5.5953e+00],\n",
      "        [6.0672e+00, 5.5770e+00],\n",
      "        [9.6315e+00, 3.7248e+00],\n",
      "        [6.2508e+00, 9.5267e+00],\n",
      "        [5.2139e+00, 2.3614e+00],\n",
      "        [1.4871e+00, 5.4243e+00],\n",
      "        [9.6532e+00, 1.2458e+00],\n",
      "        [5.4848e+00, 7.9167e+00],\n",
      "        [7.3322e+00, 6.2362e+00],\n",
      "        [5.9789e+00, 4.9764e+00],\n",
      "        [5.9731e+00, 6.6514e+00],\n",
      "        [9.6852e+00, 9.4324e+00],\n",
      "        [5.2055e+00, 9.1556e+00],\n",
      "        [4.6287e+00, 7.2472e-01],\n",
      "        [7.3945e+00, 4.9599e+00],\n",
      "        [9.1503e+00, 3.5147e+00],\n",
      "        [3.8001e+00, 8.4674e+00],\n",
      "        [6.5991e+00, 3.7637e+00],\n",
      "        [8.4796e+00, 9.3920e+00],\n",
      "        [4.7530e+00, 5.9460e+00],\n",
      "        [3.1335e+00, 1.6483e+00],\n",
      "        [1.0760e+00, 2.4094e+00],\n",
      "        [9.1706e+00, 8.3776e+00],\n",
      "        [9.6068e+00, 9.5770e+00],\n",
      "        [7.7956e+00, 3.3171e+00],\n",
      "        [4.3129e+00, 6.2796e+00],\n",
      "        [4.4547e+00, 2.1998e+00],\n",
      "        [7.3432e+00, 1.9754e+00],\n",
      "        [1.0599e+00, 1.1707e+00],\n",
      "        [9.6338e+00, 2.2455e+00],\n",
      "        [5.8921e+00, 6.6992e+00],\n",
      "        [2.9959e+00, 9.5013e+00],\n",
      "        [1.3866e+00, 9.3821e+00],\n",
      "        [1.6565e+00, 8.8383e+00],\n",
      "        [1.7563e+00, 3.3159e+00],\n",
      "        [7.7887e+00, 5.0467e+00],\n",
      "        [2.5845e+00, 5.7289e+00],\n",
      "        [7.9248e+00, 9.0048e+00],\n",
      "        [9.3040e+00, 2.0221e+00],\n",
      "        [5.2926e+00, 7.4785e+00],\n",
      "        [2.8381e+00, 8.0060e+00],\n",
      "        [6.3268e+00, 3.1934e+00],\n",
      "        [2.0256e+00, 4.9735e+00],\n",
      "        [4.9235e-01, 6.0283e+00],\n",
      "        [6.7909e+00, 9.0793e+00],\n",
      "        [3.0094e-01, 8.9000e+00],\n",
      "        [2.7567e+00, 6.9932e+00],\n",
      "        [7.3293e+00, 9.6873e+00],\n",
      "        [5.3658e+00, 9.7411e+00],\n",
      "        [8.2338e+00, 8.6769e+00],\n",
      "        [9.0952e+00, 8.3461e+00],\n",
      "        [3.4502e+00, 8.7318e+00],\n",
      "        [5.7842e+00, 8.9510e+00],\n",
      "        [5.7288e+00, 4.8436e+00],\n",
      "        [3.2135e+00, 3.4289e+00],\n",
      "        [2.0717e+00, 7.7967e-01],\n",
      "        [3.9111e+00, 6.6344e+00],\n",
      "        [5.1561e-01, 4.2850e+00],\n",
      "        [5.6133e+00, 3.4005e+00],\n",
      "        [4.6268e+00, 4.0195e+00],\n",
      "        [2.5143e-01, 4.9088e+00],\n",
      "        [6.0629e+00, 7.9019e+00],\n",
      "        [6.4863e+00, 4.0151e-01],\n",
      "        [9.8837e+00, 2.8021e+00],\n",
      "        [7.6623e+00, 2.3900e+00],\n",
      "        [5.4281e+00, 3.5474e+00],\n",
      "        [5.3287e+00, 6.9012e+00],\n",
      "        [9.3815e+00, 7.9815e+00],\n",
      "        [2.4970e+00, 5.4846e+00],\n",
      "        [2.4929e+00, 9.4892e+00],\n",
      "        [8.0068e+00, 3.4135e+00],\n",
      "        [4.6164e+00, 4.0003e+00],\n",
      "        [1.4240e+00, 6.1912e+00],\n",
      "        [3.5489e+00, 4.6745e-01],\n",
      "        [8.2618e+00, 2.3447e+00],\n",
      "        [6.0407e+00, 4.0080e+00],\n",
      "        [6.0618e+00, 9.0414e+00],\n",
      "        [1.0557e+00, 9.6954e+00],\n",
      "        [4.4387e+00, 8.3442e+00],\n",
      "        [1.7194e+00, 7.5098e+00],\n",
      "        [6.5991e+00, 1.6574e+00],\n",
      "        [5.5212e+00, 3.1299e+00],\n",
      "        [9.2746e+00, 4.6754e-01],\n",
      "        [9.7478e-01, 7.6245e+00],\n",
      "        [3.1382e+00, 8.6808e+00],\n",
      "        [4.0449e+00, 2.7167e+00],\n",
      "        [4.8027e+00, 6.2322e-02],\n",
      "        [6.0145e+00, 9.6754e+00],\n",
      "        [5.5193e+00, 2.8547e-01],\n",
      "        [5.7322e+00, 3.5749e-01],\n",
      "        [6.9076e+00, 5.0457e+00],\n",
      "        [4.9814e-01, 1.4723e+00],\n",
      "        [1.4659e+00, 5.4493e+00],\n",
      "        [8.0649e+00, 8.3436e+00],\n",
      "        [4.1064e+00, 5.8881e+00],\n",
      "        [3.5068e+00, 8.9460e+00],\n",
      "        [3.7752e+00, 9.6750e+00],\n",
      "        [2.0511e+00, 2.5470e+00],\n",
      "        [8.7349e+00, 3.3192e+00],\n",
      "        [6.4646e+00, 9.3052e-01],\n",
      "        [9.1152e+00, 2.5698e-01],\n",
      "        [7.7550e+00, 8.6754e+00],\n",
      "        [7.3259e+00, 1.1913e+00],\n",
      "        [6.1622e+00, 2.7705e+00],\n",
      "        [2.9664e+00, 1.6382e+00],\n",
      "        [6.9439e+00, 6.7722e+00],\n",
      "        [6.5498e+00, 6.0354e+00],\n",
      "        [9.9696e+00, 8.6008e+00],\n",
      "        [3.7312e+00, 4.6950e+00],\n",
      "        [5.4789e+00, 7.9912e+00],\n",
      "        [2.0946e+00, 7.9481e+00],\n",
      "        [2.6951e+00, 4.0806e+00],\n",
      "        [1.1241e+00, 3.2780e+00],\n",
      "        [2.7166e+00, 7.0678e+00]])\n",
      "tensor([-5.5768e-01,  2.6501e-01,  1.1504e+00,  1.4979e+00,  1.4566e+00,\n",
      "        -1.7815e+00,  1.5334e+00,  4.3782e-02,  1.3176e+00,  1.7969e+00,\n",
      "        -1.8827e+00,  1.6977e+00, -7.4837e-01,  9.3689e-01,  4.9047e-01,\n",
      "         1.1031e+00,  5.9454e-01, -8.9320e-01,  9.3152e-01, -7.9189e-01,\n",
      "        -1.5818e+00,  1.6252e+00,  1.2897e+00,  1.0787e+00,  2.3301e-01,\n",
      "         5.7136e-02, -3.5317e-01,  7.8053e-01, -1.1934e-01,  7.7554e-01,\n",
      "         1.2111e+00, -8.1551e-01, -1.4960e+00, -1.1502e+00, -3.4632e-01,\n",
      "        -2.6913e-01,  5.3499e-01, -9.3943e-01, -1.9442e-01, -2.7253e-01,\n",
      "         8.7536e-02, -2.3505e-01,  6.2990e-01,  8.5421e-01, -4.4785e-01,\n",
      "         1.9248e-02, -1.3896e+00,  1.5884e+00, -1.7227e+00,  1.1456e+00,\n",
      "         1.0354e+00,  4.5568e-01, -6.2768e-01, -3.7508e-02,  1.3309e+00,\n",
      "         1.6416e+00, -7.2998e-01,  8.1026e-01, -1.2664e-01, -1.8403e-01,\n",
      "        -1.3048e+00, -1.8543e+00, -2.0048e-01,  6.3134e-01, -2.9955e-03,\n",
      "         8.2644e-01,  1.1749e+00, -5.0819e-01,  1.5441e-01, -1.5025e-01,\n",
      "        -1.2869e+00,  1.4753e+00,  4.6120e-01,  1.9229e+00, -9.1912e-01,\n",
      "        -1.9229e-01, -7.9346e-01, -1.6918e+00,  3.6128e-01,  1.3838e+00,\n",
      "         1.9678e+00,  8.8996e-01, -2.8455e-01,  1.4738e+00, -5.3404e-01,\n",
      "        -1.8266e+00, -1.5245e+00,  1.3081e+00,  1.8287e+00,  5.8742e-02,\n",
      "        -6.6926e-01,  9.1913e-01, -2.8893e-01, -2.4311e-01,  1.1517e+00,\n",
      "         1.6679e+00, -2.4457e-01, -4.8540e-02,  8.4968e-01, -8.4330e-01,\n",
      "         1.0613e+00, -1.5796e-01,  8.6732e-01,  3.5694e-01,  4.8694e-01,\n",
      "         5.8345e-01, -6.3598e-01, -2.0783e-01,  1.7499e+00,  1.5303e-01,\n",
      "        -7.0884e-03, -1.3672e+00,  1.6984e+00,  2.7742e-01, -1.3258e+00,\n",
      "        -3.9046e-01,  1.2673e+00,  6.1726e-01,  1.3709e+00,  1.4354e+00,\n",
      "         1.9346e+00,  6.1246e-02, -3.5334e-02,  1.9008e+00,  5.2883e-01,\n",
      "         5.2297e-01,  1.9260e-01,  8.8537e-01,  8.4367e-02, -5.9638e-01,\n",
      "         1.2867e+00,  2.8764e-01,  5.3086e-01, -2.8680e-01,  1.4941e+00,\n",
      "        -2.0686e-01, -1.0108e+00,  2.5384e-01,  1.2631e+00,  1.7119e+00,\n",
      "        -1.7836e-01,  1.6809e+00,  3.1794e-01,  1.4203e-01,  6.0918e-01,\n",
      "        -8.8880e-01, -8.9609e-01,  2.4206e-01,  2.5788e-01, -1.4710e+00,\n",
      "         2.8544e-01,  1.1231e+00, -9.0678e-01,  1.5410e+00, -2.4253e-01,\n",
      "         5.2449e-01, -6.1321e-01, -3.8375e-03, -1.2924e+00,  1.1704e+00,\n",
      "         1.8030e+00,  1.1522e-02, -1.8060e+00, -7.8889e-01, -1.7558e+00,\n",
      "        -1.8300e+00, -7.1999e-01, -1.3977e+00,  1.6471e+00,  1.2846e+00,\n",
      "         2.2743e-01, -1.4886e+00, -8.3908e-01,  4.0576e-02, -5.7633e-01,\n",
      "         5.0012e-01, -1.2106e+00, -1.3584e+00,  1.8179e+00, -5.4041e-02,\n",
      "        -4.9332e-01, -3.0152e-02,  1.9435e+00,  1.4229e+00, -1.2683e-01,\n",
      "         1.0113e+00, -1.1688e+00, -1.8862e-01,  1.1096e+00,  1.4602e+00,\n",
      "         6.9175e-01, -6.5085e-01,  6.1039e-01, -2.3600e-01,  1.1732e-01,\n",
      "        -1.1480e-01,  1.8234e-01, -1.6188e+00, -4.6676e-01,  9.4444e-01,\n",
      "        -2.8883e-01, -1.1585e+00, -6.0385e-02,  1.8057e+00,  9.4380e-01,\n",
      "        -1.8204e+00,  1.3627e+00, -1.2306e+00,  1.2044e+00, -8.5434e-02,\n",
      "         2.1180e-02,  2.9993e-01, -6.0393e-01,  6.8236e-03, -1.5589e-01,\n",
      "        -9.5312e-01, -5.8557e-01,  7.6122e-01, -1.8877e+00, -1.4258e+00,\n",
      "        -1.9041e+00, -1.1787e+00,  5.6034e-01,  1.8510e-02, -7.7923e-01,\n",
      "         8.7974e-01,  7.6904e-01, -2.0184e-01, -7.4622e-01,  1.8393e+00,\n",
      "         4.4320e-01,  3.6902e-01, -6.4337e-01,  1.0434e-03,  1.7707e+00,\n",
      "         5.6934e-01, -1.7040e+00,  1.6299e+00,  1.1335e-01, -1.8787e+00,\n",
      "        -4.1007e-02,  7.3192e-01, -1.7733e+00, -1.3403e-01, -8.2689e-01,\n",
      "         1.3491e-01, -1.0441e+00, -4.7049e-01, -1.4415e+00, -3.0543e-01,\n",
      "         2.4784e-01, -1.4321e+00,  1.8859e+00, -1.9195e+00, -1.0288e-01,\n",
      "         5.0853e-02, -9.4523e-01,  8.8086e-01,  5.6484e-01,  2.0388e-01,\n",
      "         7.0923e-01,  7.7486e-03, -6.2711e-01, -4.1951e-01,  6.5139e-02,\n",
      "         8.4019e-01,  1.4228e+00, -1.3905e-01, -1.8356e+00,  1.0669e+00,\n",
      "         9.1565e-01,  1.4488e+00, -3.3913e-03,  7.2382e-01, -1.4421e+00,\n",
      "         8.8402e-01,  1.3715e-01,  8.4683e-01, -1.9954e+00, -1.6872e+00,\n",
      "         1.3853e+00, -1.2634e+00,  1.1886e+00, -1.7728e+00, -7.5996e-01,\n",
      "        -1.0198e+00, -5.8913e-01, -6.8460e-01,  1.5348e+00,  1.8871e+00,\n",
      "        -8.3890e-01,  8.8025e-02, -7.1480e-01, -3.3520e-01, -1.9131e+00,\n",
      "         3.2016e-01, -1.9396e+00, -1.1502e-01, -3.1244e-01,  1.0236e+00,\n",
      "        -2.1849e-01,  1.6308e+00,  1.7809e+00,  2.2706e-01,  2.8316e-01,\n",
      "         1.9940e+00,  7.1426e-01, -4.8575e-01,  1.3488e+00,  3.1669e-03,\n",
      "         9.9729e-01,  1.0436e+00,  5.5090e-01,  1.4664e+00, -1.2271e+00,\n",
      "         1.7606e+00, -1.9611e+00, -9.9471e-01, -5.1604e-02,  1.0219e+00,\n",
      "         1.9903e+00, -1.6551e+00,  4.9045e-01,  3.6769e-01, -1.0486e+00,\n",
      "        -5.9356e-02, -4.9802e-01, -2.0424e-01,  6.0754e-02,  4.5977e-01,\n",
      "         1.2000e-01, -1.4452e+00,  1.5863e-01,  2.2752e-01,  1.2218e+00,\n",
      "         2.6617e-02,  9.4604e-02, -5.3478e-01,  1.0434e+00,  1.9690e-01,\n",
      "        -2.7209e-01, -4.2111e-01, -5.8932e-01, -3.3231e-01, -7.9366e-01,\n",
      "        -1.2005e-01, -1.5996e+00, -1.8047e-01,  6.2679e-01,  3.4631e-01,\n",
      "        -6.9451e-01, -4.1345e-01, -8.4122e-02, -1.5802e-01,  6.7055e-01,\n",
      "         9.4054e-01, -3.3577e-01, -6.2466e-01,  9.0418e-02,  1.6152e+00,\n",
      "         3.4316e-01,  1.1329e-01,  2.4506e-01, -2.4189e-01,  1.0072e+00,\n",
      "        -1.1350e+00,  1.2757e+00, -1.6090e+00,  3.2377e-01,  4.8134e-02,\n",
      "        -1.4659e-01, -1.5860e-01,  3.3958e-01,  3.1576e-01,  4.7850e-01,\n",
      "         1.2085e+00,  8.1582e-02,  7.8037e-02,  7.3736e-01,  6.7611e-01,\n",
      "         1.0911e+00,  8.0620e-01,  5.6292e-01,  6.3592e-01, -1.0690e+00,\n",
      "         8.1320e-01,  3.3584e-01,  5.4652e-01, -1.0399e+00, -1.0272e+00,\n",
      "        -1.5876e+00,  1.6498e+00,  9.2905e-02, -7.7889e-01,  1.8658e+00,\n",
      "        -3.8714e-02,  6.2783e-01, -1.2574e+00, -1.8449e+00, -2.4782e-01,\n",
      "         1.1413e+00, -6.6018e-01, -1.1876e+00, -5.0197e-01, -1.8886e-01,\n",
      "        -5.5472e-02, -6.9286e-02,  1.3635e-01, -2.4853e-01, -1.1694e+00,\n",
      "         1.3669e-02,  7.8739e-02, -1.5553e+00,  4.7874e-01,  1.2619e+00,\n",
      "        -8.3220e-01,  5.3356e-01, -8.5185e-01, -1.5999e-02,  1.6342e-01,\n",
      "        -1.9960e-03,  1.3260e+00,  1.3790e+00,  8.4384e-02, -3.1564e-01,\n",
      "        -4.6962e-01,  1.4742e-01, -9.5508e-01,  1.1565e+00,  1.4404e+00,\n",
      "        -4.5471e-01, -5.6903e-01,  1.1338e+00, -1.0028e-01, -1.7444e+00,\n",
      "         1.9558e-01, -1.4887e-01, -1.0731e+00, -1.3683e+00, -3.9559e-01,\n",
      "        -1.0309e+00,  1.5883e+00,  2.4317e-01,  7.8534e-02, -1.5876e+00,\n",
      "        -1.6351e+00,  4.4397e-01, -2.6636e-01,  1.1222e+00, -1.3859e+00,\n",
      "         2.5105e-01, -1.6734e+00, -9.7370e-04, -8.3951e-02,  1.2986e+00,\n",
      "        -3.9379e-01,  2.5080e-02, -1.6488e+00,  1.9850e+00,  4.9661e-01,\n",
      "         2.1905e-01, -8.8775e-01, -1.1470e+00, -9.3394e-02, -1.4336e+00,\n",
      "         1.3264e+00,  2.2421e-01, -1.6903e+00,  1.0423e+00,  1.0551e+00,\n",
      "        -7.3239e-01, -1.6964e+00,  2.1369e-03, -1.2342e+00,  2.6781e-01,\n",
      "         4.1326e-01,  9.1185e-01,  5.7615e-01,  1.6665e+00,  5.0758e-01,\n",
      "         1.0104e-01, -1.2447e+00, -1.5609e+00,  5.8504e-02, -3.4779e-01,\n",
      "         7.7786e-01,  1.2718e+00,  2.6302e-01,  1.2343e+00, -1.0527e+00,\n",
      "         1.0692e-01,  1.4964e+00,  1.2329e+00, -1.1976e+00, -5.7344e-01,\n",
      "        -8.5716e-01,  7.7192e-01, -1.5879e-01, -8.8841e-02,  1.1199e+00])\n"
     ]
    }
   ],
   "source": [
    "print(train_x)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4fd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e877dcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: torch.Size([500, 2])\n",
      "train_y shape: torch.Size([500, 1])\n",
      "tensor([[2.8540e-03, 1.9346e-02],\n",
      "        [9.9945e+00, 9.8661e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors for testing\n",
    "train_x = torch.tensor(np.column_stack((x_coordinates, y_coordinates)), dtype=torch.float32)  # Mock input data\n",
    "train_y = torch.sin(train_x[:, 0]) + torch.cos(train_x[:, 1])  # Mock target values based on some function of X, Y\n",
    "train_y = train_y.unsqueeze(-1)\n",
    "\n",
    "print(f\"train_x shape: {train_x.shape}\")\n",
    "print(f\"train_y shape: {train_y.shape}\")\n",
    "\n",
    "\n",
    "# Define bounds for optimization - based on the X and Y ranges in the dataset\n",
    "bounds = torch.tensor([\n",
    "    [train_x[:, 0].min(), train_x[:, 1].min()],\n",
    "    [train_x[:, 0].max(), train_x[:, 1].max()]\n",
    "])\n",
    "\n",
    "print(bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6295c626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in optimization function: An explicit output dimension is required for targets. Expected Y with dimension: 1 (got 2).\n"
     ]
    }
   ],
   "source": [
    "def simulate_bayesian_optimization_step(train_x, train_y, bounds, num_candidates=2):\n",
    "    model = SingleTaskGP(train_x, train_y)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_torch(mll)  # Fit GP model to data\n",
    "\n",
    "    acq_func = qExpectedImprovement(\n",
    "        model=model,\n",
    "        best_f=train_y.max(),\n",
    "        sampler=SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "    )\n",
    "\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=num_candidates,\n",
    "        num_restarts=5,\n",
    "        raw_samples=10,\n",
    "    )\n",
    "    return candidates\n",
    "\n",
    "try:\n",
    "        # Normalize input and output data\n",
    "    train_x_normalized = (train_x - train_x.mean(dim=0)) / train_x.std(dim=0)\n",
    "    train_y_normalized = (train_y - train_y.mean()) / train_y.std()\n",
    "\n",
    "    # Use normalized data in optimization function\n",
    "    candidates = simulate_bayesian_optimization_step(train_x_normalized, train_y_normalized, bounds)\n",
    "    print(\"Candidates:\", candidates)\n",
    "except Exception as e:\n",
    "    print(f\"Error in optimization function: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_bayesian_optimization_step(train_x, train_y, bounds, num_candidates=2):\n",
    "    model = SingleTaskGP(train_x, train_y)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    \n",
    "    # Define Expected Improvement acquisition function\n",
    "    acq_func = qExpectedImprovement(\n",
    "        model=model,\n",
    "        best_f=train_y.max(),\n",
    "        sampler=SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "    )\n",
    "\n",
    "    # Optimize acquisition function to get candidate points\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=num_candidates,\n",
    "        num_restarts=5,\n",
    "        raw_samples=10,\n",
    "    )\n",
    "    return candidates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bosampler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
