{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c257aa",
   "metadata": {},
   "source": [
    "# botorch_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b51a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "from botorch.fit import fit_gpytorch_torch\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.test_functions import Hartmann\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.generation import get_best_candidates, gen_candidates_torch\n",
    "from botorch.optim import gen_batch_initial_conditions\n",
    "from botorch.acquisition import qExpectedImprovement, qUpperConfidenceBound\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, NoReturn, Optional, Tuple, Type, Union\n",
    "from botorch.acquisition.acquisition import AcquisitionFunction\n",
    "from botorch.acquisition.objective import (\n",
    "    IdentityMCObjective,\n",
    "    MCAcquisitionObjective,\n",
    "    PosteriorTransform,\n",
    ")\n",
    "from botorch.models.model import Model\n",
    "from botorch.sampling.samplers import MCSampler, SobolQMCNormalSampler\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "from botorch.utils.transforms import (\n",
    "    concatenate_pending_points,\n",
    "    match_batch_shape,\n",
    "    t_batch_mode_transform,\n",
    ")\n",
    "from botorch.acquisition.monte_carlo import MCAcquisitionFunction\n",
    "from inspect import signature\n",
    "\n",
    "\n",
    "import time\n",
    "from typing import Any, Dict, List, NamedTuple, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from gpytorch.mlls.marginal_log_likelihood import MarginalLogLikelihood\n",
    "from torch import Tensor\n",
    "from torch.optim.adam import Adam\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "def check_convergence(\n",
    "    loss_trajectory: List[float],\n",
    "    param_trajectory: Dict[str, List[Tensor]],\n",
    "    options: Dict[str, Union[float, str]],\n",
    ") -> bool:\n",
    "    r\"\"\"Check convergence of optimization for pytorch optimizers.\n",
    "\n",
    "    Right now this is just a dummy function and only checks for maxiter.\n",
    "\n",
    "    Args:\n",
    "        loss_trajectory: A list containing the loss value at each iteration.\n",
    "        param_trajectory: A dictionary mapping each parameter name to a list of Tensors\n",
    "            where the `i`th Tensor is the parameter value at iteration `i`.\n",
    "        options: dictionary of options. Currently only \"maxiter\" is supported.\n",
    "\n",
    "    Returns:\n",
    "        A boolean indicating whether optimization has converged.\n",
    "    \"\"\"\n",
    "    maxiter: int = options.get(\"maxiter\", 50)\n",
    "    # TODO: Be A LOT smarter about this\n",
    "    # TODO: Make this work in batch mode (see parallel L-BFGS-P)\n",
    "    if len(loss_trajectory) >= maxiter:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def _filter_kwargs(function: Callable, **kwargs: Any) -> Any:\n",
    "    r\"\"\"Filter out kwargs that are not applicable for a given function.\n",
    "    Return a copy of given kwargs dict with only the required kwargs.\"\"\"\n",
    "    return {k: v for k, v in kwargs.items() if k in signature(function).parameters}\n",
    "\n",
    "def _get_extra_mll_args(\n",
    "    mll: MarginalLogLikelihood\n",
    ") -> Union[List[Tensor], List[List[Tensor]]]:\n",
    "    r\"\"\"Obtain extra arguments for MarginalLogLikelihood objects.\n",
    "\n",
    "    Get extra arguments (beyond the model output and training targets) required\n",
    "    for the particular type of MarginalLogLikelihood for a forward pass.\n",
    "\n",
    "    Args:\n",
    "        mll: The MarginalLogLikelihood module.\n",
    "\n",
    "    Returns:\n",
    "        Extra arguments for the MarginalLogLikelihood.\n",
    "    \"\"\"\n",
    "    if isinstance(mll, ExactMarginalLogLikelihood):\n",
    "        return list(mll.model.train_inputs)\n",
    "    elif isinstance(mll, SumMarginalLogLikelihood):\n",
    "        return [list(x) for x in mll.model.train_inputs]\n",
    "    elif isinstance(mll, VariationalELBO):\n",
    "        return []\n",
    "    else:\n",
    "        raise ValueError(\"Do not know how to optimize MLL type.\")\n",
    "\n",
    "\n",
    "\n",
    "ParameterBounds = Dict[str, Tuple[Optional[float], Optional[float]]]\n",
    "\n",
    "\n",
    "class OptimizationIteration(NamedTuple):\n",
    "    itr: int\n",
    "    fun: float\n",
    "    time: float\n",
    "\n",
    "\n",
    "\n",
    "def fit_gpytorch_torch(\n",
    "    mll: MarginalLogLikelihood,\n",
    "    bounds: Optional[ParameterBounds] = None,\n",
    "    optimizer_cls: Optimizer = Adam,\n",
    "    options: Optional[Dict[str, Any]] = None,\n",
    "    track_iterations: bool = True,\n",
    ") -> Tuple[MarginalLogLikelihood, List[OptimizationIteration]]:\n",
    "    r\"\"\"Fit a gpytorch model by maximizing MLL with a torch optimizer.\n",
    "\n",
    "    The model and likelihood in mll must already be in train mode.\n",
    "    Note: this method requires that the model has `train_inputs` and `train_targets`.\n",
    "\n",
    "    Args:\n",
    "        mll: MarginalLogLikelihood to be maximized.\n",
    "        bounds: A ParameterBounds dictionary mapping parameter names to tuples\n",
    "            of lower and upper bounds. Bounds specified here take precedence\n",
    "            over bounds on the same parameters specified in the constraints\n",
    "            registered with the module.\n",
    "        optimizer_cls: Torch optimizer to use. Must not require a closure.\n",
    "        options: options for model fitting. Relevant options will be passed to\n",
    "            the `optimizer_cls`. Additionally, options can include: \"disp\"\n",
    "            to specify whether to display model fitting diagnostics and \"maxiter\"\n",
    "            to specify the maximum number of iterations.\n",
    "        track_iterations: Track the function values and wall time for each\n",
    "            iteration.\n",
    "\n",
    "    Returns:\n",
    "        2-element tuple containing\n",
    "\n",
    "        - mll with parameters optimized in-place.\n",
    "        - List of OptimizationIteration objects with information on each\n",
    "          iteration. If track_iterations is False, this will be an empty list.\n",
    "\n",
    "    Example:\n",
    "        >>> gp = SingleTaskGP(train_X, train_Y)\n",
    "        >>> mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "        >>> mll.train()\n",
    "        >>> fit_gpytorch_torch(mll)\n",
    "        >>> mll.eval()\n",
    "    \"\"\"\n",
    "    optim_options = {\"maxiter\": 100, \"disp\": True, \"lr\": 0.05}\n",
    "    optim_options.update(options or {})\n",
    "    optimizer = optimizer_cls(\n",
    "        params=[{\"params\": mll.parameters()}],\n",
    "        **_filter_kwargs(optimizer_cls, **optim_options),\n",
    "    )\n",
    "\n",
    "    # get bounds specified in model (if any)\n",
    "    bounds_: ParameterBounds = {}\n",
    "    if hasattr(mll, \"named_parameters_and_constraints\"):\n",
    "        for param_name, _, constraint in mll.named_parameters_and_constraints():\n",
    "            if constraint is not None and not constraint.enforced:\n",
    "                bounds_[param_name] = constraint.lower_bound, constraint.upper_bound\n",
    "\n",
    "    # update with user-supplied bounds (overwrites if already exists)\n",
    "    if bounds is not None:\n",
    "        bounds_.update(bounds)\n",
    "\n",
    "    iterations = []\n",
    "    t1 = time.time()\n",
    "\n",
    "    param_trajectory: Dict[str, List[Tensor]] = {\n",
    "        name: [] for name, param in mll.named_parameters()\n",
    "    }\n",
    "    loss_trajectory: List[float] = []\n",
    "    i = 0\n",
    "    converged = False\n",
    "    train_inputs, train_targets = mll.model.train_inputs, mll.model.train_targets\n",
    "    while not converged:\n",
    "        optimizer.zero_grad()\n",
    "        output = mll.model(*train_inputs)\n",
    "\n",
    "        # we sum here to support batch mode\n",
    "        args = [output, train_targets] + _get_extra_mll_args(mll)\n",
    "        loss = -mll(*args).sum()\n",
    "        loss.backward()\n",
    "        loss_trajectory.append(loss.item())\n",
    "        for name, param in mll.named_parameters():\n",
    "            param_trajectory[name].append(param.detach().clone())\n",
    "        if optim_options[\"disp\"] and (\n",
    "            (i + 1) % 10 == 0 or i == (optim_options[\"maxiter\"] - 1)\n",
    "        ):\n",
    "            print(f\"Iter {i + 1}/{optim_options['maxiter']}: {loss.item()}\")\n",
    "        if track_iterations:\n",
    "            iterations.append(OptimizationIteration(i, loss.item(), time.time() - t1))\n",
    "        optimizer.step()\n",
    "        # project onto bounds:\n",
    "        if bounds_:\n",
    "            for pname, param in mll.named_parameters():\n",
    "                if pname in bounds_:\n",
    "                    param.data = param.data.clamp(*bounds_[pname])\n",
    "        i += 1\n",
    "        converged = check_convergence(\n",
    "            loss_trajectory=loss_trajectory,\n",
    "            param_trajectory=param_trajectory,\n",
    "            options={\"maxiter\": optim_options[\"maxiter\"]},\n",
    "        )\n",
    "    return mll, iterations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clamp_tensor(tensor: torch.Tensor, bounds: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Clamps the values of a tensor within given bounds.\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor): The tensor to clamp.\n",
    "        bounds (Tensor): A tensor containing the lower and upper bounds.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The clamped tensor.\n",
    "    \"\"\"\n",
    "    if bounds.dim() == 2 and bounds.size(0) == 2:\n",
    "        # Common bounds for all samples: bounds shape is (2, dim)\n",
    "        lower_bounds, upper_bounds = bounds\n",
    "    elif bounds.dim() == 3 and bounds.size(1) == 2:\n",
    "        # Per-sample bounds: bounds shape is (batch_size, 2, dim)\n",
    "        lower_bounds = bounds[:, 0, :]\n",
    "        upper_bounds = bounds[:, 1, :]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid bounds dimension. Expected bounds of shape (2, dim) or (batch_size, 2, dim).\")\n",
    "    \n",
    "    # Perform clamping\n",
    "    return torch.max(torch.min(tensor, upper_bounds), lower_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_initial_candidates(bounds: torch.Tensor, num_candidates: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generates initial candidate points uniformly within the given bounds.\n",
    "\n",
    "    Args:\n",
    "        bounds (Tensor): A tensor containing the lower and upper bounds.\n",
    "            - If bounds has shape (2, dim), it represents common bounds for all samples.\n",
    "            - If bounds has shape (batch_size, 2, dim), it represents per-sample bounds.\n",
    "        num_candidates (int): The number of candidates to generate.\n",
    "            - For per-sample bounds, num_candidates must equal batch_size.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A tensor of initial candidate points with shape (num_candidates, dim).\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if bounds.dim() == 2 and bounds.size(0) == 2:\n",
    "        # Common bounds for all samples\n",
    "        dim = bounds.shape[1]\n",
    "        lower_bounds, upper_bounds = bounds  # Shapes: (dim,)\n",
    "        # Generate random points uniformly within bounds\n",
    "        random_numbers = torch.rand(num_candidates, dim)\n",
    "        initial_points = lower_bounds + (upper_bounds - lower_bounds) * random_numbers\n",
    "    elif bounds.dim() == 3 and bounds.size(1) == 2:\n",
    "        # Per-sample bounds\n",
    "        batch_size = bounds.shape[0]\n",
    "        dim = bounds.shape[2]\n",
    "        if num_candidates != batch_size:\n",
    "            raise ValueError(\"For per-sample bounds, num_candidates must equal batch_size.\")\n",
    "        lower_bounds = bounds[:, 0, :]  # Shape: (batch_size, dim)\n",
    "        upper_bounds = bounds[:, 1, :]  # Shape: (batch_size, dim)\n",
    "        # Generate random points uniformly within per-sample bounds\n",
    "        random_numbers = torch.rand(num_candidates, dim)\n",
    "        initial_points = lower_bounds + (upper_bounds - lower_bounds) * random_numbers\n",
    "    else:\n",
    "        raise ValueError(\"Invalid bounds shape. Expected shape (2, dim) or (batch_size, 2, dim).\")\n",
    "    return initial_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_acquisition_function(\n",
    "    acquisition_function: AcquisitionFunction,\n",
    "    bounds: Tensor,\n",
    "    num_restarts: int,\n",
    "    raw_samples: int,\n",
    "    num_candidates: int,\n",
    "    optimizer_options: Optional[Dict[str, Any]] = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Optimizes the acquisition function to find the next set of candidates.\n",
    "\n",
    "    Args:\n",
    "        acquisition_function (AcquisitionFunction): The acquisition function to optimize.\n",
    "        bounds (Tensor): A tensor containing the lower and upper bounds.\n",
    "        num_restarts (int): Number of restarts for optimization.\n",
    "        raw_samples (int): Number of raw samples to use for initialization.\n",
    "        num_candidates (int): Number of candidates to generate.\n",
    "        optimizer_options (Optional[Dict[str, Any]]): Options for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The optimized candidate points.\n",
    "    \"\"\"\n",
    "    from botorch.optim.optimize import optimize_acqf\n",
    "\n",
    "    # Use BoTorch's built-in optimizer for acquisition functions\n",
    "    # https://botorch.org/api/optim.html\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acquisition_function,\n",
    "        bounds=bounds,\n",
    "        q=num_candidates,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "        options=optimizer_options or {},\n",
    "    )\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ca1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_acquisition_function(\n",
    "    acquisition_function: AcquisitionFunction,\n",
    "    bounds: Tensor,\n",
    "    num_restarts: int,\n",
    "    raw_samples: int,\n",
    "    num_candidates: int,\n",
    "    optimizer_options: Optional[Dict[str, Any]] = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Optimizes the acquisition function to find the next set of candidates.\n",
    "\n",
    "    Args:\n",
    "        acquisition_function (AcquisitionFunction): The acquisition function to optimize.\n",
    "        bounds (Tensor): A tensor containing the lower and upper bounds.\n",
    "        num_restarts (int): Number of restarts for optimization.\n",
    "        raw_samples (int): Number of raw samples to use for initialization.\n",
    "        num_candidates (int): Number of candidates to generate.\n",
    "        optimizer_options (Optional[Dict[str, Any]]): Options for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The optimized candidate points.\n",
    "    \"\"\"\n",
    "    from botorch.optim.optimize import optimize_acqf\n",
    "\n",
    "    # Use BoTorch's built-in optimizer for acquisition functions\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acquisition_function,\n",
    "        bounds=bounds,\n",
    "        q=num_candidates,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "        options=optimizer_options or {},\n",
    "    )\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_step_bayesian_optimization(\n",
    "    train_x: Tensor,\n",
    "    train_y: Tensor,\n",
    "    bounds: Tensor,\n",
    "    num_candidates: int,\n",
    "    acquisition_function_type: str = 'EI',\n",
    "    acq_function_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    optimizer_options: Optional[Dict[str, Any]] = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Performs one step of Bayesian Optimization to propose new candidate points.\n",
    "\n",
    "    Args:\n",
    "        train_x (Tensor): Training input data.\n",
    "        train_y (Tensor): Training target data.\n",
    "        bounds (Tensor): A tensor containing the lower and upper bounds.\n",
    "        num_candidates (int): Number of candidates to propose.\n",
    "        acquisition_function_type (str): Type of acquisition function ('EI' or 'UCB').\n",
    "        acq_function_kwargs (Optional[Dict[str, Any]]): Additional arguments for the acquisition function.\n",
    "        optimizer_options (Optional[Dict[str, Any]]): Options for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The proposed candidate points.\n",
    "    \"\"\"\n",
    "    # Fit a Gaussian Process model\n",
    "    model = SingleTaskGP(train_x, train_y)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    \n",
    "\n",
    "    \n",
    "    fit_gpytorch_torch(mll)\n",
    "\n",
    "    # Define the acquisition function\n",
    "    if acquisition_function_type == 'EI':\n",
    "        acq_func = qExpectedImprovement(\n",
    "            model=model,\n",
    "            best_f=train_y.max(),\n",
    "            sampler=SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "            **(acq_function_kwargs or {})\n",
    "        )\n",
    "    elif acquisition_function_type == 'UCB':\n",
    "        acq_func = qUpperConfidenceBound(\n",
    "            model=model,\n",
    "            beta=acq_function_kwargs.get('beta', 0.1),\n",
    "            sampler=SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported acquisition function type.\")\n",
    "\n",
    "    # Optimize the acquisition function\n",
    "    candidates = optimize_acquisition_function(\n",
    "        acquisition_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        num_restarts=10,\n",
    "        raw_samples=100,\n",
    "        num_candidates=num_candidates,\n",
    "        optimizer_options=optimizer_options,\n",
    "    )\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fa913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomExpectedImprovement(MCAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        best_f: Union[float, Tensor],\n",
    "        xi: float = 0.0,\n",
    "        sampler: Optional[MCSampler] = None,\n",
    "        objective: Optional[MCAcquisitionObjective] = None,\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            sampler=sampler or SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "            objective=objective,\n",
    "            posterior_transform=posterior_transform,\n",
    "        )\n",
    "        self.best_f = best_f\n",
    "        self.xi = xi\n",
    "\n",
    "    @concatenate_pending_points\n",
    "    @t_batch_mode_transform()\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        posterior = self.model.posterior(X)\n",
    "        samples = self.sampler(posterior)\n",
    "        obj = self.objective(samples) if self.objective else samples\n",
    "        improvement = obj - self.best_f - self.xi\n",
    "        return improvement.clamp_min(0).mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be64158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUpperConfidenceBound(MCAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        beta: float = 0.1,\n",
    "        sampler: Optional[MCSampler] = None,\n",
    "        objective: Optional[MCAcquisitionObjective] = None,\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            sampler=sampler or SobolQMCNormalSampler(sample_shape=torch.Size([500])),\n",
    "            objective=objective,\n",
    "            posterior_transform=posterior_transform,\n",
    "        )\n",
    "        self.beta = beta\n",
    "\n",
    "    @concatenate_pending_points\n",
    "    @t_batch_mode_transform()\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        posterior = self.model.posterior(X)\n",
    "        samples = self.sampler(posterior)\n",
    "        obj = self.objective(samples) if self.objective else samples\n",
    "        mean = obj.mean(dim=0)\n",
    "        std_dev = obj.std(dim=0)\n",
    "        return mean + self.beta * std_dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c6e5a",
   "metadata": {},
   "source": [
    "# botorch_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312015ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# botorch_fit.py\n",
    "\n",
    "import time\n",
    "from typing import Optional, Dict, Any, List, NamedTuple, Tuple\n",
    "import torch\n",
    "from torch.optim import Optimizer, Adam\n",
    "from gpytorch.mlls.marginal_log_likelihood import MarginalLogLikelihood\n",
    "from gpytorch import settings as gpt_settings\n",
    "\n",
    "class OptimizationIteration(NamedTuple):\n",
    "    iteration: int\n",
    "    loss: float\n",
    "    wall_time: float\n",
    "\n",
    "def fit_gpytorch_torch(\n",
    "    mll: MarginalLogLikelihood,\n",
    "    optimizer_cls: Optimizer = Adam,\n",
    "    options: Optional[Dict[str, Any]] = None,\n",
    "    track_iterations: bool = True,\n",
    "    approx_mll: bool = True,\n",
    ") -> Tuple[MarginalLogLikelihood, List[OptimizationIteration]]:\n",
    "    \"\"\"\n",
    "    Fits a GPyTorch model by maximizing the Marginal Log Likelihood (MLL) using a PyTorch optimizer.\n",
    "\n",
    "    Args:\n",
    "        mll (MarginalLogLikelihood): The MLL to be maximized.\n",
    "        optimizer_cls (Optimizer): The PyTorch optimizer class to use (default is Adam).\n",
    "        options (Optional[Dict[str, Any]]): Options for model fitting and the optimizer.\n",
    "            Relevant options include:\n",
    "                - \"lr\": Learning rate for the optimizer.\n",
    "                - \"maxiter\": Maximum number of iterations.\n",
    "                - \"disp\": If True, displays optimization progress.\n",
    "        track_iterations (bool): If True, tracks the function values and wall time for each iteration.\n",
    "        approx_mll (bool): If True, uses approximate MLL computation for efficiency.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - mll: The MLL with optimized parameters.\n",
    "            - iterations: A list of OptimizationIteration objects with information on each iteration.\n",
    "    \"\"\"\n",
    "    options = options or {}\n",
    "    lr = options.get(\"lr\", 0.1)\n",
    "    maxiter = options.get(\"maxiter\", 50)\n",
    "    disp = options.get(\"disp\", False)\n",
    "    exclude = options.get(\"exclude\", None)\n",
    "\n",
    "    # Prepare parameters for optimization\n",
    "    if exclude is not None:\n",
    "        mll_params = [\n",
    "            t for p_name, t in mll.named_parameters() if p_name not in exclude\n",
    "        ]\n",
    "    else:\n",
    "        mll_params = list(mll.parameters())\n",
    "\n",
    "    optimizer = optimizer_cls(\n",
    "        params=[{\"params\": mll_params}],\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    iterations = []\n",
    "    t_start = time.time()\n",
    "    param_trajectory: Dict[str, List[torch.Tensor]] = {\n",
    "        name: [] for name, param in mll.named_parameters()\n",
    "    }\n",
    "    loss_trajectory: List[float] = []\n",
    "\n",
    "    mll.train()\n",
    "    train_inputs = mll.model.train_inputs\n",
    "    train_targets = mll.model.train_targets\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        optimizer.zero_grad()\n",
    "        with gpt_settings.fast_computations(log_prob=approx_mll):\n",
    "            output = mll.model(*train_inputs)\n",
    "            # Sum over batch dimensions for compatibility\n",
    "            loss = -mll(output, train_targets).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        loss_trajectory.append(loss_value)\n",
    "        for name, param in mll.named_parameters():\n",
    "            param_trajectory[name].append(param.detach().clone())\n",
    "\n",
    "        if disp and ((i + 1) % 10 == 0 or i == maxiter - 1):\n",
    "            print(f\"Iter {i + 1}/{maxiter}: Loss = {loss_value:.4f}\")\n",
    "\n",
    "        if track_iterations:\n",
    "            iterations.append(\n",
    "                OptimizationIteration(\n",
    "                    iteration=i,\n",
    "                    loss=loss_value,\n",
    "                    wall_time=time.time() - t_start,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    mll.eval()\n",
    "    return mll, iterations, param_trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbaca8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wesam\\anaconda3\\envs\\bosampler\\lib\\site-packages\\gpytorch\\utils\\cholesky.py:38: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0502e+00, 3.5680e+00],\n",
       "        [2.2273e-03, 3.8682e+00]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define simulation parameters\n",
    "num_frames = 100  # Number of frames in the video sequence\n",
    "num_pedestrians = 5  # Number of unique pedestrians\n",
    "\n",
    "# Generate mock data\n",
    "frame_numbers = np.repeat(np.arange(1, num_frames + 1), num_pedestrians)\n",
    "pedestrian_ids = np.tile(np.arange(1, num_pedestrians + 1), num_frames)\n",
    "x_coordinates = np.random.uniform(0, 10, size=num_frames * num_pedestrians)  # Random X coordinates\n",
    "y_coordinates = np.random.uniform(0, 10, size=num_frames * num_pedestrians)  # Random Y coordinates\n",
    "\n",
    "# Create DataFrame\n",
    "pedestrian_data = pd.DataFrame({\n",
    "    \"Frame Number\": frame_numbers,\n",
    "    \"Pedestrian ID\": pedestrian_ids,\n",
    "    \"X-Coordinate\": x_coordinates,\n",
    "    \"Y-Coordinate\": y_coordinates\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "pedestrian_data.head()\n",
    "\n",
    "# Now set up for testing Bayesian optimization on this data\n",
    "# Convert X and Y coordinates as sample input data for optimization function\n",
    "\n",
    "# Convert data to PyTorch tensors for testing\n",
    "train_x = torch.tensor(np.column_stack((x_coordinates, y_coordinates)), dtype=torch.float32)  # Mock input data\n",
    "train_y = torch.sin(train_x[:, 0]) + torch.cos(train_x[:, 1])\n",
    " # Mock target values based on some function of X, Y\n",
    "\n",
    "# Define bounds for optimization - based on the X and Y ranges in the dataset\n",
    "bounds = torch.tensor([\n",
    "    [train_x[:, 0].min(), train_x[:, 1].min()],\n",
    "    [train_x[:, 0].max(), train_x[:, 1].max()]\n",
    "])\n",
    "\n",
    "# Function to simulate Bayesian Optimization step using provided code\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import qExpectedImprovement\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "import gpytorch\n",
    "    # Fit a Gaussian Process model\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "def simulate_bayesian_optimization_step(train_x, train_y, bounds, num_candidates=2):\n",
    "\n",
    "\n",
    "    likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1e-5))\n",
    "    model = SingleTaskGP(train_x, train_y.unsqueeze(-1), likelihood=likelihood)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with gpytorch.settings.cholesky_jitter(1e-1):\n",
    "        fit_gpytorch_torch(mll)\n",
    "\n",
    "    # Define Expected Improvement acquisition function\n",
    "    from botorch.sampling import SobolQMCNormalSampler\n",
    "\n",
    "    \n",
    "    \n",
    "    acq_func = qExpectedImprovement(\n",
    "        model=model,\n",
    "        best_f=train_y.max(),\n",
    "        sampler=SobolQMCNormalSampler(num_samples=500),\n",
    "    )\n",
    "\n",
    "\n",
    "    # Optimize acquisition function to get candidate points\n",
    "    from botorch.optim.optimize import optimize_acqf\n",
    "\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=num_candidates,\n",
    "        num_restarts=10,\n",
    "        raw_samples=20,\n",
    "    )\n",
    "    return candidates\n",
    "\n",
    "train_x = (train_x - train_x.mean(dim=0)) / train_x.std(dim=0)\n",
    "train_y = (train_y - train_y.mean()) / train_y.std()\n",
    "\n",
    "# Perform a single Bayesian Optimization step\n",
    "candidates = simulate_bayesian_optimization_step(train_x, train_y, bounds)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c2476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bosampler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
